{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Network\n",
    "\n",
    "http://adventuresinmachinelearning.com/python-tensorflow-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Tensorflow example 1\n",
    "\n",
    "Objective: Calculate $a = (b+c) * (c+2)$. \n",
    "\n",
    "This function can be broken down to:\n",
    "$$d = b+c$$\n",
    "$$e = c+2$$\n",
    "$$a = d* e$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare _constants_ and _variables_. The first argument is the value to be assigned; the second argument is optional string name (useful for later visualizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwan/.virtualenvs/dlcv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kwan/.virtualenvs/dlcv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kwan/.virtualenvs/dlcv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kwan/.virtualenvs/dlcv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kwan/.virtualenvs/dlcv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kwan/.virtualenvs/dlcv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create constant\n",
    "const = tf.constant(2.0, name=\"const\")\n",
    "\n",
    "# create variables\n",
    "b = tf.Variable(2.0, name='b')\n",
    "c = tf.Variable(1.0, name='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once variables and constants are declared, create _operations_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create operations\n",
    "d = tf.add(b, c, name='d')\n",
    "e = tf.add(c, const, name='e')\n",
    "a = tf.multiply(d, e, name='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set up an object to initialize variables and graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup variable init\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start TensorFlow session `tf.Session` to run operations. \n",
    "\n",
    "By having defined the variables and operations, we have created a graph structure. TF Session is an object to compile and build the graph structure. Run the graph by using Python `with` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable a is 9.0\n"
     ]
    }
   ],
   "source": [
    "# start session\n",
    "with tf.Session() as sess:\n",
    "    # init variables\n",
    "    sess.run(init_op)\n",
    "    # compute output of graph\n",
    "    a_out = sess.run(a)\n",
    "    print(\"Variable a is {}\".format(a_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of example 1.**\n",
    "\n",
    "Objective: Calculate $a = (b+c) * (c+2)$. \n",
    "\n",
    "This function can be broken down to:\n",
    "$$d = b+c$$\n",
    "$$e = c+2$$\n",
    "$$a = d* e$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Placeholder\n",
    "\n",
    "Take the same situation as above, but what if we didn't know the value of *b*? In the previous example, $b = 2.0$ and $c = 1.0$ and this was declared as variables.\n",
    "\n",
    "In the case where *b* is unknown, `tf.placeholder` can be used to declare the basic data structure when declaring the variable.\n",
    "\n",
    "Below, the first argument declares the data type for the element in the tensor - *float32*. The second argument sets the shape of the data to be \"injected\" into the variable - *(?x1) array*. The *None* declared in the size allows us to inject as much 1-d data as we want into *b*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create constant\n",
    "const = tf.constant(2.0, name=\"const\")\n",
    "\n",
    "# create variables\n",
    "b = tf.placeholder(tf.float32, [None, 1], name='b')\n",
    "c = tf.Variable(1.0, name='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create operations\n",
    "d = tf.add(b, c, name='d')\n",
    "e = tf.add(c, const, name='e')\n",
    "a = tf.multiply(d, e, name='a')\n",
    "\n",
    "# setup variable init\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to run the operation with a TensorFlow Session, we have to make sure we feed in the value for *b*. This is done in `a_out`.\n",
    "\n",
    "Below, we specify that the array is to be a 1d range from 0 to 10. To use `feed_dict`, one must supply a Python dictionary, with the placeholder name as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable a is [[ 3.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " [15.]\n",
      " [18.]\n",
      " [21.]\n",
      " [24.]\n",
      " [27.]\n",
      " [30.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    a_out = sess.run(a, feed_dict={b: np.arange(0, 10)[:, np.newaxis]})\n",
    "    print(\"Variable a is {}\".format(a_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Neural Network\n",
    "\n",
    "Creating a simple three layer neural network, then applying it to the MNIST dataset provided by TensorFlow.\n",
    "\n",
    "About MNIST dataset:\n",
    "- 28x28 px grayscale images of hand-written digits\n",
    "- 55,000 training rows\n",
    "- 10,000 testing rows\n",
    "- 5,000 validation rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "First, load the data. The `one_hot` argument specifies if labels should be one hot encoded. Eg. \"4\" is [0, 0, 0, 0 ,1, 0, 0, 0, 0, 0]. One hot encoding allows us to easily feed it to the output layer of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-5e54a559e0c0>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/kwan/.virtualenvs/dlcv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/kwan/.virtualenvs/dlcv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/kwan/.virtualenvs/dlcv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/kwan/.virtualenvs/dlcv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/kwan/.virtualenvs/dlcv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.models.official.mnist import dataset\n",
    "mnist = input_data.read_data_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup placeholders\n",
    "\n",
    "To begin setting up the graph structure, setup the placeholder variables for the training data and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python optimization variables - training params\n",
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# declare training data placeholder\n",
    "# input x - 28x28 px = 784 nodes\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# output data placeholder (labels - 10 digits)\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup weight and bias variables\n",
    "\n",
    "Setup weight and bias variables for the three layer NN. The number of weights/bias tensors is always $L-1$, where $L$ is the number of layers. This means that we need to set up 2 tensors for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare weights/bias connecting input to hidden layer\n",
    "W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([300]), name='b1')\n",
    "# declare weights/bias connecting hidden layer to output layer\n",
    "W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be 300 nodes in the hidden layer, so the size of W1 is [784, 300], as there are 784 inputs (28x28 pixels).\n",
    "\n",
    "`tf.random_normal` is used to initialize the weight values with a random normal distribution, with 0 mean and 0.03 standard deviation. It creates a matrix of the given size, populating it with random samples drawn from the given distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup hidden layer\n",
    "\n",
    "Setup node inputs and activation functions of hidden layer nodes by executing the following equations:\n",
    "\n",
    "$$z^{(l+1)} = W^{(l)}x + b^{(l)}$$\n",
    "$$h^{(l+1)} = f(z^{(l+1)})$$\n",
    "\n",
    "The first equation multiplies the weight with the input vector and adds the bias. Use `tf.matmul` to execute matrix multiplication.\n",
    "\n",
    "The second equation applies the ReLU (rectified linear unit) activation function. Use the built in ReLU from TensorFlow - `tf.nn.relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate output of hidden layer\n",
    "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup output layer\n",
    "\n",
    "Setup output layer *y_* by multiplying the weight with the hidden layer output and adding the second bias variable. Here we use softmax activation for the output layer with `tf.nn.softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate hidden layer output - softmax activated output layer\n",
    "y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost/loss function\n",
    "\n",
    "Use cross entropy cost function:\n",
    "$$ J = - \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} y_j^{(i)} log(y_{j-}^{(i)}) + (1 - y_j^{(i)}) log(1 - y_{j-} ^{(i)})$$\n",
    "\n",
    "where: \n",
    "- $y_j^{(i)}$ is the *i* th training label for output node *j*,\n",
    "- $y_{j-}^{(i)}$ is the *i* th predicted label for output node *j*,\n",
    "- *m* is the number of training / batch samples,\n",
    "- *n* is the number\n",
    "\n",
    "The first summation (n): of logarithmic products and additions of all output nodes.\n",
    "\n",
    "The second summation (m): mean of first summation across all training samples.\n",
    "\n",
    "The cross-entropy loss will output a probability value between 0 and 1. This value indicates the performance of classification. The lower the loss, the better the performance (more accurate in predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"clip_by_value_2:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                        + (1-y) * tf.log(1-y_clipped), axis=1))\n",
    "\n",
    "print(y_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_clipped` is a clipped version of `y_`. This is to prevent $log(0)$ from happening during training. This would return `NaN` and break the training process. With `y_clipped`, the cross entropy is calculated. \n",
    "\n",
    "`tf.reduce_sum` takes the sum of the cross-entropy for a single node and training sample. As y_clipped is (m x 10), the sum is done across the second axis, hence `axis=1` (Python uses zero-based indexing). Suppose y_clipped was to be (10 x m), then the sum would have to be done across the first axis (`axis=0`).\n",
    "\n",
    "`tf.reduce_mean` takes the mean of the tensor and completes the cross entropy cost calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup optimizer\n",
    "\n",
    "Use the gradient descent optimizer. This is provided by TensorFlow. Initialize it with the learning rate then minimize the cost operation. The function will perform gradient descent and backpropagation.\n",
    "\n",
    "Before running the operationis, setup the operator object, as well as the operation for evaluating the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_1:0\", shape=(?,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# setup initialization operator\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# define accuracy assessment operation\n",
    "correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.equal` function returns a boolean depending on the given argument. `tf.argmax` returns the index of the max value in the tensor (like numpy with vectors). Thus, `correct_pred` returns values of whether the digit is correctly predicted (shape: m x 1) per sample.\n",
    "\n",
    "Before being able to calculate the mean for the correct predictions with `tf.reduce_mean`. The boolean is transformed to a float using `tf.cast`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Start session and run the training in batches. \n",
    "\n",
    "Use TensorFlow's `next_batch` to extract batches of the training data. This is a function provided by the MNIST dataset for extracting randomized batches of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 0.588\n",
      "Epoch: 2 cost = 0.210\n",
      "Epoch: 3 cost = 0.151\n",
      "Epoch: 4 cost = 0.120\n",
      "Epoch: 5 cost = 0.097\n",
      "Epoch: 6 cost = 0.078\n",
      "Epoch: 7 cost = 0.063\n",
      "Epoch: 8 cost = 0.051\n",
      "Epoch: 9 cost = 0.040\n",
      "Epoch: 10 cost = 0.033\n",
      "Training complete\n",
      "0.9783\n"
     ]
    }
   ],
   "source": [
    "# start session\n",
    "with tf.Session() as sess:\n",
    "    # init variables\n",
    "    sess.run(init_op)\n",
    "    total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _, c = sess.run([optimizer, cross_entropy],\n",
    "                           feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch+1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "    print(\"Training complete\")\n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the batch samples, two operations are then run (within the same line). These are the optimizer operation and the cross entropy. The outputs are assigned to `_` and `c`. Batch samples (x and y) are injected into both operations.\n",
    "\n",
    "The output from the optimizer is not important. The cross_entropy output is what is needed to calculate the average cost per epoch.\n",
    "\n",
    "Once training is complete, the accuracy operation is run, injecting test images and labels to get the prediction accuracy on the test set, which is approximately 98%.\n",
    "\n",
    "To improve the model, regularization could be added."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcv",
   "language": "python",
   "name": "dlcv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
